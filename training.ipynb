{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46109c49-b5af-41dc-9867-0260a8f48500",
   "metadata": {},
   "source": [
    "# Sentiment aware embeddings\n",
    "The focus of this repo is to investigate the possibility to create sentiment aware embeddings. What I mean by this is a system that produces strong positive embeddings for a pair of query term and sentence where the sentence is positive towards the query term, and negative if it expresses a negative sentiment. For example:\n",
    "\n",
    "positive (cosine close to 1):\n",
    "query term: Coca Cola\n",
    "sentence: Love coca cola - best drink ever!\n",
    "\n",
    "negative (cosine close to -1):\n",
    "query term: Coca Cola\n",
    "sentence: Hate coca cola - tastes like pepsi, just way worse!\n",
    "\n",
    "One potential use case for such encoders would be for vector search within brand management. With this system you can vectorise and upload all sentences that mentions either your brand or a competitor to a vector database and then search for sentences that are postive or negative towards your brand or one of your competitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "046817bf-4550-489c-8b9a-6fa30d290ea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0+cu118)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.5.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.0.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.34.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.45.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2024.6.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (3.10.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (0.25.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 4)) (5.9.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 4)) (0.4.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (0.20.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 3)) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 3)) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b8345d5-fc0e-4a53-94b7-9b8f22c6cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from model import TargetedSentimentEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6887fa19-5176-4136-94ce-b7e2e96672b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ffebb6f-3717-4dde-9292-a081861101f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('fhamborg/news_sentiment_newsmtsc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c674e4c8-2c20-4285-bc95-7786fa06c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetedSentimentEncoder(nn.Module):\n",
    "    def __init__(self, base_model: str):\n",
    "        super(TargetedSentimentEncoder, self).__init__()\n",
    "        \n",
    "        # Separate BERT encoders for query and sentence\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "        self.query_encoder = AutoModel.from_pretrained(base_model)\n",
    "        self.text_encoder = AutoModel.from_pretrained(base_model)\n",
    "        \n",
    "        # Freeze all layers except the last encoder stack for both networks\n",
    "        #self._freeze_encoder(self.query_encoder)\n",
    "        #self._freeze_encoder(self.text_encoder)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def _freeze_encoder(self, encoder):\n",
    "        \"\"\"\n",
    "        Freeze all layers except the last encoder stack.\n",
    "        \"\"\"\n",
    "        for name, param in encoder.named_parameters():\n",
    "            # Check if the layer belongs to the last encoder stack\n",
    "            if 'layer' in name and 'layer.5' not in name:  # DistilBERT has 6 layers, we freeze up to layer 4\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, query_text, sentence_text):\n",
    "        # Tokenize input\n",
    "        query_inputs = self.tokenizer(query_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        sentence_inputs = self.tokenizer(sentence_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        # Get last hidden states for both query and sentence inputs\n",
    "        query_hidden_states = self.query_encoder(**query_inputs).last_hidden_state  # Shape: [batch_size, seq_len, hidden_dim]\n",
    "        sentence_hidden_states = self.text_encoder(**sentence_inputs).last_hidden_state  # Shape: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        # Apply max pooling across the sequence (dim=1)\n",
    "        query_embeds, _ = torch.max(query_hidden_states, dim=1)  # Shape: [batch_size, hidden_dim]\n",
    "        sentence_embeds, _ = torch.max(sentence_hidden_states, dim=1)  # Shape: [batch_size, hidden_dim]\n",
    "        \n",
    "        # Dropout layer\n",
    "        query_embeds = self.dropout(query_embeds)\n",
    "        sentence_embeds = self.dropout(sentence_embeds)\n",
    "        \n",
    "        return query_embeds, sentence_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1c2de9c-89bd-4660-939a-52ee0c978fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TanhLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TanhLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, query_embeds, sentence_embeds, target):\n",
    "        # Compute cosine similarity between query and sentence embeddings\n",
    "        cos_sim = F.cosine_similarity(query_embeds, sentence_embeds)\n",
    "        \n",
    "        # Optionally apply tanh to the cosine similarity\n",
    "        pred_tanh = torch.tanh(cos_sim)\n",
    "        \n",
    "        # Compute the difference between tanh of the prediction and the target\n",
    "        loss = torch.mean((pred_tanh - target) ** 2)  # Mean squared error\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44cfbbfb-4ec1-48ad-820d-dbce8d08ed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at cross-encoder/nli-roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cross-encoder/nli-roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = 'cross-encoder/nli-roberta-base'\n",
    "model = TargetedSentimentEncoder(base_model=base_model, device=device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "#criterion = nn.CosineEmbeddingLoss()\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = TanhLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e92023d9-664b-4ed5-be1c-1bc7b575b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomTextPairDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hf_dataset: Hugging Face dataset, e.g., train, test, or validation split.\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the 'mention' (query_sentence), 'sentence' (text_sentence), and 'polarity' (label)\n",
    "        query_sentence = self.dataset[idx]['mention']\n",
    "        text_sentence = self.dataset[idx]['sentence']\n",
    "        label = torch.tensor(self.dataset[idx]['polarity'], dtype=torch.float)  # Assuming polarity is already -1, 0, or 1\n",
    "        return query_sentence, text_sentence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c776ffe-5af6-4aed-ae48-d14f205aa77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset objects for train, test, and validation splits\n",
    "train_dataset = CustomTextPairDataset(data['train'])\n",
    "test_dataset = CustomTextPairDataset(data['test'])\n",
    "validation_dataset = CustomTextPairDataset(data['validation'])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "882012fc-30b7-4786-9c16-10243b3b17bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [01:22<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.3942\n",
      "Epoch [1/10], Test Loss: 0.5318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [01:21<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 0.2718\n",
      "Epoch [2/10], Test Loss: 0.3289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [01:22<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss: 0.2332\n",
      "Epoch [3/10], Test Loss: 0.3618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [01:22<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Training Loss: 0.2119\n",
      "Epoch [4/10], Test Loss: 0.4754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [01:22<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Training Loss: 0.1955\n",
      "Epoch [5/10], Test Loss: 0.3463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [01:22<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Training Loss: 0.1843\n",
      "Epoch [6/10], Test Loss: 0.4007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [01:22<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Training Loss: 0.1710\n",
      "Epoch [7/10], Test Loss: 0.3389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [01:22<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Training Loss: 0.1700\n",
      "Epoch [8/10], Test Loss: 0.3744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [01:22<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Training Loss: 0.1620\n",
      "Epoch [9/10], Test Loss: 0.3509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [01:21<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Training Loss: 0.1555\n",
      "Epoch [10/10], Test Loss: 0.3404\n"
     ]
    }
   ],
   "source": [
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop for 10 epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0  # Initialize the total loss for the epoch\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        query_text, sentence_text, label = batch  # Get the inputs from the dataloader\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        query_embeds, sentence_embeds = model(query_text, sentence_text)\n",
    "\n",
    "        # Compute similarity and loss\n",
    "        similarity = cosine_similarity(query_embeds, sentence_embeds)\n",
    "        loss = criterion(label, similarity)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Average training loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # Print loss for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0  # Initialize the validation loss\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for batch in test_dataloader:  # Iterate over validation data\n",
    "            query_text, sentence_text, label = batch  # Get the inputs\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            query_embeds, sentence_embeds = model(query_text, sentence_text)\n",
    "\n",
    "            # Compute similarity and loss\n",
    "            similarity = cosine_similarity(query_embeds, sentence_embeds)\n",
    "            loss = criterion(label, similarity)\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Average validation loss for the epoch\n",
    "    avg_val_loss = val_loss / len(test_dataloader)\n",
    "    \n",
    "    # Print validation loss for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e8c7d8e-5ed4-425d-b8a8-09666a03f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbfd09b5-ffc8-48a9-90fd-1717ad48f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# List to store predictions and actual labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# No gradient computation needed for evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        query_text, sentence_text, label = batch  # Get the inputs from the dataloader\n",
    "        \n",
    "        # Forward pass to get cosine similarity scores\n",
    "        query_embeds, sentence_embeds = model(query_text, sentence_text)\n",
    "        \n",
    "        # Compute cosine similarity between query and sentence embeddings\n",
    "        cos_sim = torch.nn.functional.cosine_similarity(query_embeds, sentence_embeds)\n",
    "\n",
    "        # Convert cosine similarity to discrete values (-1, 0, 1)\n",
    "        pred_labels = torch.where(cos_sim > 0.5, 1, torch.where(cos_sim < -0.5, -1, 0))\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        predictions.extend(pred_labels.cpu().numpy())  # Move to CPU for easy handling\n",
    "        true_labels.extend(label.cpu().numpy())        # Also move labels to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91d2dd6f-0a8e-48d7-ab27-224445a83c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ae96b5e-3627-414f-9573-4e4566609aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.73      0.82      0.77       295\n",
      "         0.0       0.72      0.70      0.71       319\n",
      "         1.0       0.78      0.66      0.72       189\n",
      "\n",
      "    accuracy                           0.74       803\n",
      "   macro avg       0.74      0.73      0.73       803\n",
      "weighted avg       0.74      0.74      0.73       803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
